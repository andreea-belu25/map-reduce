Used structures:
---
- map_data: consists of data shared between all the map threads 
- map_thread: consists of data specific only to a map thread
- reduce_data: consists of data shared between all the reduce threads
- reduce_thread: consists of data specific only to a reduce thread

Functions:
---
    - main():
       - parse the input: number of map threads, reduce threads, as well as the file containing the files to process
       - extract the files to be processed, and store them in a vector
       - initialize the shared data structures required by all the threads
       - initialize and start the map threads, as well as the reduce threads
       - join the results from the threads.
       - finally, destroy all the synchronization variables used

    - map_function() called by all map threads:
       - while there are unprocessed files, takes a file and process it
       - use a mutex while extracting the next file to process, in order to prevent other threads from accessing the same file
       - use a barrier, in order to yield the reduce threads while the map threads are still processing

   - process_file():
       - reads each word from the file, and transforms it, removing all non-character symbols, as well as converting uppercase to lowercase
       - store the words in an unordered map
       - finally, transform the unordered map into a partial list in which the elements are pairs of type {word, file_id}
       - while writing the partial list into the shared memory, use mutex in order to avoid overwriting between threads

    - reduce_function() called by all reduce threads:
       - use a barrier, in order to yield until all the map threads have finished processing
       - aggregate the partial lists generated by the mapping function
       - use another barrier, in order to yield until the aggregation has completed
       - finally, write the output files

    - aggregate_words():
       - split the pairs evenly among the reduce threads
       - store the results for each thread in an unordered map of form: {word: {file_id_1, file_id_2, ...}} (the word and the associated files ids)
                for each pair in the interval designated to the thread, check if the word is already in the map,
                    if it is, append the file id to the list associated with the word
                    otherwise, create the a new entry in the map 

        - update the shared final list, which is an unordered map of form: {letter: {word: {file_id_1, file_id_2, ...}}}
        - use a mutex while updating the list, in order to avoid overwriting between threads.
                for each word in the resulting unordered map, add it in the final list at the key equal to its first letter
                    if there is already a word there, concatenate the file ids

    - write_files():
        - store all possible letters in an array
        - threads will extract the letter for which to write the output file in a similar fashion to how map threads extracted the file names to process
        - convert the unordered map stored at that entry associated with each letter into an array
        - sort the array based on the number of file ids each word has, break ties by comparing words alphabetically
        - format the output and write the output file


A general aspect is that everywhere I had to read or write in a file I have made an auxiliar check to verify if there were problems when opening
the files.
